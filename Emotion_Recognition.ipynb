{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Recognition.ipynb",
      "provenance": [],
      "mount_file_id": "1BSAs2h_sNw0iwylMmeVY37EqvqbMicPr",
      "authorship_tag": "ABX9TyPxdvf7xzpiWjPjcnW+bcaT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWJygdU61NuC",
        "outputId": "08ed361e-6c98-43a2-e3b6-8360c142c320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional,LSTM,GRU,Dense\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "nltk.download('punkt')\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('train.txt','r')\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_train.append(l[1].strip())\n",
        "    x_train.append(l[0])\n",
        "f=open('test.txt','r')\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "f=open('val.txt','r')\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "data_train=pd.DataFrame({'Text':x_train,'Emotion':y_train})\n",
        "data_test=pd.DataFrame({'Text':x_test,'Emotion':y_test})\n",
        "data=data_train.append(data_test,ignore_index=True)"
      ],
      "metadata": {
        "id": "N7VNDI7W1v2h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing all the prepositions, articles, punctuation marks, stop words, leaving only the important words in the sentences."
      ],
      "metadata": {
        "id": "hurUXi283Uk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(data):\n",
        "    data=re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
        "    data=re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
        "    data=word_tokenize(data)\n",
        "    return data\n",
        "texts=[' '.join(clean_text(text)) for text in data.Text]\n",
        "texts_train=[' '.join(clean_text(text)) for text in x_train]\n",
        "texts_test=[' '.join(clean_text(text)) for text in x_test]"
      ],
      "metadata": {
        "id": "JXQyMgtA3X0J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizing each sentence, extracting each unique word and createing a dictionary where each unique word is assigned an index."
      ],
      "metadata": {
        "id": "ivalg8Ix3bw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequence_train=tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test=tokenizer.texts_to_sequences(texts_test)\n",
        "index_of_words=tokenizer.word_index\n",
        "vocab_size=len(index_of_words)+1"
      ],
      "metadata": {
        "id": "Qg5a3X9K3gqv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data-set which were obtained has six unique results or emotions, namely: anger, sadness, fear, joy, surprise and love.\n",
        "\n",
        "Each emotion is given a category value (0–5) in later phases. The 'encoding' dictionary and the 'to categorical' function are used for this reason. We translate the categorical value back to the emotion while trying to get a result."
      ],
      "metadata": {
        "id": "-yuMFDFo3kmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=6\n",
        "embed_num_dims=300\n",
        "max_seq_len=500\n",
        "class_names=['anger','sadness','fear','joy','surprise','love']\n",
        "X_train_pad=pad_sequences(sequence_train,maxlen=max_seq_len)\n",
        "X_test_pad=pad_sequences(sequence_test,maxlen=max_seq_len)\n",
        "encoding={'anger':0,'sadness':1,'fear':2,'joy':3,'surprise':4,'love':5}\n",
        "y_train=[encoding[x] for x in data_train.Emotion]\n",
        "y_test=[encoding[x] for x in data_test.Emotion]\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "metadata": {
        "id": "TXesXTq13pKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the creation of this model, i used a set of word vectors.\n",
        "\n",
        "Using these word vectors allows us to train our model more quickly and thoroughly, resulting in improved training accuracy."
      ],
      "metadata": {
        "id": "hK4K-Qpk5LkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(filepath,word_index,embedding_dim):\n",
        "    vocab_size=len(word_index)+1\n",
        "    embedding_matrix=np.zeros((vocab_size,embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word,*vector=line.split()\n",
        "            if word in word_index:\n",
        "                idx=word_index[word]\n",
        "                embedding_matrix[idx] = np.array(vector,dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "fname='word_vectors.vec'\n",
        "embedd_matrix=create_embedding_matrix(fname,index_of_words,embed_num_dims)"
      ],
      "metadata": {
        "id": "wS_OGDCP4KX_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'm working on an architecture for the model's training. To accomplish this, I first developed a **Embedding layer**, whose weights are derived from the word vectors file.\n",
        "\n",
        "\n",
        "I also added a **Bidirectional layer**, whose features are:\n",
        "gru_output_size = 128\n",
        "dropout = 0.2\n",
        "recurrent_dropout = 0.2\n",
        "\n",
        "\n",
        "Finally, a **Dense layer** is added which has ‘softmax’ activation.\n",
        "Adam’s optimizer is used as the optimizer and loss is calculated using ‘categorical_crossentropy’.\n",
        "\n",
        "*‘model.summary()’ can be used to see the features, layer type, output shape and number of parameters in the model.*"
      ],
      "metadata": {
        "id": "-EZ2eSgk9g5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedd_layer=Embedding(vocab_size,embed_num_dims,input_length=max_seq_len,weights=[embedd_matrix],trainable=False)\n",
        "gru_output_size=128\n",
        "bidirectional=True\n",
        "model=Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(Bidirectional(GRU(units=gru_output_size,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yks4OTU04cuH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I train the model using the training set and test the accuracy simultaneously, since the metric of the model is set to ‘accuracy’.\n",
        "\n",
        "Here, the batch size is taken as 128 and number of epochs is 8. (The batch size and number of epochs can be changed)"
      ],
      "metadata": {
        "id": "QFtfsm6s-QAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "epochs=8\n",
        "hist=model.fit(X_train_pad,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test_pad,y_test))"
      ],
      "metadata": {
        "id": "Xa-7vKdf4fMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After 8 epochs are completed, I wil test the model next."
      ],
      "metadata": {
        "id": "li0g2gvP-dlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message=['I am sad.']\n",
        "seq=tokenizer.texts_to_sequences(message)\n",
        "padded=pad_sequences(seq,maxlen=max_seq_len)\n",
        "pred=model.predict(padded)\n",
        "print('Message:'+str(message))\n",
        "print('Emotion:',class_names[np.argmax(pred)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5LscGnU4hFX",
        "outputId": "b2dfae49-0085-4578-a121-99adb3228d83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message:['I am sad.']\n",
            "Emotion: anger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the above sentence, I got the result as ‘sadness’, which is most probably correct."
      ],
      "metadata": {
        "id": "go_1XL2Y-qn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(model,'textmodel',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFmVmPcJ4ikO",
        "outputId": "3eb6ea84-c19b-44de-d519-eb0b639b612a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: textmodel/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: textmodel/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f36d7499590> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f36d74669d0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The possibilities are unlimited when using this text model to develop a quiz that identifies the user's emotion or perform sentiment analysis of tweets or Reddit posts etc. here in this case it's a mood detecting recipie reccomendation system."
      ],
      "metadata": {
        "id": "RAauiSNA-zRs"
      }
    }
  ]
}